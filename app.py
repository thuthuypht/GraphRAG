import streamlit as st
import pandas as pd
import os
import random
import re

from modules.data_handler import load_ontology, load_authors_list
from modules.feature_extraction import extract_features_for_pair
from modules.model_handler import load_model, predict_link_prob, suggest_collaborators

st.set_page_config(layout="wide")
# --- CUSTOM CSS INJECTION FOR SIDEBAR WIDTH (ƒê√£ kh√¥i ph·ª•c) ---
# ƒê·∫∑t chi·ªÅu r·ªông sidebar b·∫±ng 25% ƒë·ªô r·ªông m√†n h√¨nh
st.markdown("""
<style>
/* Selector targeting the main sidebar container */
section[data-testid="stSidebar"] {
    /* ƒê·∫∑t chi·ªÅu r·ªông sidebar b·∫±ng 25% ƒë·ªô r·ªông m√†n h√¨nh */
    width: 30% !important; 
    transition: width 0.3s ease-in-out;
}
</style>
""", unsafe_allow_html=True)
# -----------------------------------------------------------

# --- H√†m s·ª≠ d·ª•ng Caching ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t ---

@st.cache_resource(show_spinner="ƒêang t·∫£i Ontology...")
def load_cached_ontology(path):
    """
    N·∫°p file ontology v√† l∆∞u v√†o b·ªô nh·ªõ cache.
    """
    print(f"T·∫£i ontology t·ª´: {path}")
    if path:
        try:
            return load_ontology(path)
        except Exception as e:
            st.error(f"ƒê√£ x·∫£y ra l·ªói khi n·∫°p Ontology: {e}. Vui l√≤ng ki·ªÉm tra l·∫°i file.")
            return None
    return None

@st.cache_data(show_spinner="ƒêang tr√≠ch xu·∫•t danh s√°ch t√°c gi·∫£...")
def get_cached_author_list(onto_data):
    """
    Tr√≠ch xu·∫•t danh s√°ch t√°c gi·∫£ t·ª´ ontology v√† l∆∞u v√†o b·ªô nh·ªõ cache.
    """
    print("Tr√≠ch xu·∫•t danh s√°ch t√°c gi·∫£ t·ª´ ontology.")
    if onto_data:
        try:
            authors_full_list = load_authors_list(onto_data)
            return authors_full_list
        except Exception as e:
            st.error(f"ƒê√£ x·∫£y ra l·ªói khi tr√≠ch xu·∫•t danh s√°ch t√°c gi·∫£: {e}.")
            return []
    return []

# --- C·∫•u h√¨nh Thanh b√™n (Sidebar) ---
st.sidebar.title("Qu·∫£n l√Ω d·ªØ li·ªáu h·ªçc thu·∫≠t ‚öôÔ∏è")
st.sidebar.header("Ch·ªçn C∆° s·ªü d·ªØ li·ªáu")

# L·∫•y danh s√°ch c√°c c∆° s·ªü d·ªØ li·ªáu (th∆∞ m·ª•c con trong data)
try:
    databases = [d for d in os.listdir("data") if os.path.isdir(os.path.join("data", d))]
    selected_db = st.sidebar.selectbox("Vui l√≤ng ch·ªçn C∆° s·ªü d·ªØ li·ªáu:", options=databases)
    print(f"Th∆∞ m·ª•c c∆° s·ªü d·ªØ li·ªáu ƒë√£ ch·ªçn: {selected_db}")
except FileNotFoundError:
    st.sidebar.error("Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c 'data/'. Vui l√≤ng t·∫°o th∆∞ m·ª•c n√†y.")
    st.stop()

# L·ª±a ch·ªçn Ontology d·ª±a tr√™n c∆° s·ªü d·ªØ li·ªáu ƒë√£ ch·ªçn
ontology_path = None
selected_ontology = None
review_year = None
if selected_db:
    st.sidebar.header("Ch·ªçn Ontology")
    ONTOLOGIES_DIR = os.path.join("data", selected_db, "ontologies")
    print(f"ƒê∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß t·ªõi th∆∞ m·ª•c ontologies: {ONTOLOGIES_DIR}")
    try:
        owl_files = [f for f in os.listdir(ONTOLOGIES_DIR) if f.endswith('.owl')]
        if not owl_files:
            st.sidebar.warning(f"Kh√¥ng t√¨m th·∫•y file Ontology n√†o trong th∆∞ m·ª•c '{ONTOLOGIES_DIR}'.")
            
        selected_ontology = st.sidebar.selectbox("Vui l√≤ng ch·ªçn m·ªôt file Ontology:", options=owl_files)
        print(f"File ontology ƒë√£ ch·ªçn: {selected_ontology}")
        if selected_ontology:
            ontology_path = os.path.join(ONTOLOGIES_DIR, selected_ontology)
            print(f"ƒê∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß t·ªõi file ontology: {ontology_path}")

            # Tr√≠ch xu·∫•t nƒÉm t·ª´ t√™n file ontology
            year_match = re.search(r'\b\d{4}\b', selected_ontology)
            if year_match:
                review_year = int(year_match.group(0))
                st.sidebar.info(f"NƒÉm xem x√©t: **{review_year}**")
            else:
                st.sidebar.warning("Kh√¥ng th·ªÉ t√¨m th·∫•y nƒÉm trong t√™n file ontology.")
    except FileNotFoundError:
        st.sidebar.error(f"Th∆∞ m·ª•c '{ONTOLOGIES_DIR}' kh√¥ng t·ªìn t·∫°i.")
        selected_ontology = None
        ontology_path = None

def load_initial_ontology(path):
    """N·∫°p file ontology v√† x·ª≠ l√Ω l·ªói."""
    print(f"T·∫£i ontology t·ª´: {path}")
    if path:
        try:
            return load_ontology(path)
        except Exception as e:
            st.error(f"ƒê√£ x·∫£y ra l·ªói khi n·∫°p Ontology: {e}. Vui l√≤ng ki·ªÉm tra l·∫°i file.")
            return None
    return None

onto_data = load_initial_ontology(ontology_path)
author_names = []
# ƒê·∫£m b·∫£o danh s√°ch t√°c gi·∫£ lu√¥n ƒë∆∞·ª£c t·∫£i v√† c√≥ s·∫µn
if onto_data:
    try:
        authors_full_list = load_authors_list(onto_data)
    except Exception as e:
        st.error(f"ƒê√£ x·∫£y ra l·ªói khi tr√≠ch xu·∫•t danh s√°ch t√°c gi·∫£: {e}.")
        authors_full_list = []
else:
    authors_full_list = []
    st.info("Ch∆∞a c√≥ Ontology n√†o ƒë∆∞·ª£c n·∫°p. M·ªôt s·ªë ch·ª©c nƒÉng c√≥ th·ªÉ kh√¥ng ho·∫°t ƒë·ªông.")
if authors_full_list:
    author_names = [author.name for author in authors_full_list]
# T·∫°o m·ªôt danh s√°ch t√°c gi·∫£ ng·∫´u nhi√™n ƒë·ªÉ s·ª≠ d·ª•ng trong session state
if 'authorslist' not in st.session_state and authors_full_list:
    num_to_choose = min(20, len(author_names))
    st.session_state.authorslist = random.choices(author_names, k=num_to_choose)

# H√†m ƒë·ªÉ l√†m m·ªõi danh s√°ch t√°c gi·∫£
def reset_authors_list():
    if authors_full_list:
        num_to_choose = min(20, len(authors_full_list))
        st.session_state.authorslist = random.choices(author_names, k=num_to_choose)

authors_list = st.session_state.get('authorslist', [])
# --- Centralized Model Selection ---
st.sidebar.header("Ch·ªçn M√¥ h√¨nh")
model_choice = st.sidebar.radio("Ch·ªçn m√¥ h√¨nh", [
    "Logistic Regression (LR)",
    "Decision Tree (DT)",
    "Random Forest (RF)",
    "Multi-Layer Perceptron (MLP)",
    "Graph Convolutional Netwwork (GCN)",
])
    # --- Tham s·ªë chung cho d·ª± ƒëo√°n ---
st.sidebar.header("Tham s·ªë d·ª± ƒëo√°n")
prob_threshold = st.sidebar.slider("Ng∆∞·ª°ng x√°c su·∫•t", 0.0, 1.0, 0.5)

# --- Tab 1: D·ª± ƒëo√°n Li√™n k·∫øt ---
tab1, tab2, tab3, tab4 = st.tabs(["D·ª± ƒëo√°n li√™n k·∫øt", "Tra c·ª©u v√† g·ª£i √Ω", "Qu·∫£n tr·ªã", "Xu·∫•t b√°o c√°o"])

with tab1:
    st.header("1. D·ª± ƒëo√°n Li√™n k·∫øt ü§ù")
    # --- Logic qu·∫£n l√Ω danh s√°ch ng·∫´u nhi√™n ---
    if 'authorslist' not in st.session_state:
        if authors_full_list:
            num_to_choose = min(20, len(authors_full_list))
            authors_random = random.choices(authors_full_list, k=num_to_choose)
            st.session_state.authorslist = [author.name for author in authors_random]
        else:
            st.session_state.authorslist = []

    authors_list = st.session_state.authorslist
    def reset_authors_list():
        if 'authorslist' in st.session_state:
            del st.session_state.authorslist

    
    # --- Ch·ªçn ph∆∞∆°ng th·ª©c nh·∫≠p li·ªáu ---
    input_method = st.radio(
        "Ch·ªçn ph∆∞∆°ng th·ª©c nh·∫≠p li·ªáu",
        ("Th·ªß c√¥ng", "Ng·∫´u nhi√™n", "H√†ng lo·∫°t t·ª´ file")
    )

    author_A = None
    author_B = None
    uploaded_file = None

    if input_method == "Th·ªß c√¥ng":
        st.subheader("Nh·∫≠p li·ªáu th·ªß c√¥ng")
        author_A = st.text_input("Nh·∫≠p t√™n T√°c gi·∫£ 1", key="author_A_manual_val")
        author_B = st.text_input("Nh·∫≠p t√™n T√°c gi·∫£ 2", key="author_B_manual_val")
       
    elif input_method == "Ng·∫´u nhi√™n":
        st.subheader("Nh·∫≠p li·ªáu theo danh s√°ch ng·∫´u nhi√™n")
        
        st.button("L√†m m·ªõi danh s√°ch t√°c gi·∫£", on_click=reset_authors_list)
        
        col_a, col_b = st.columns(2)
        with col_a:
            author_A = st.selectbox(
                "Ch·ªçn T√°c gi·∫£ 1 (Ng·∫´u nhi√™n)", 
                options=authors_list,
                key="author_A_random_val"
            )
        with col_b:
            author_B = st.selectbox(
                "Ch·ªçn T√°c gi·∫£ 2 (Ng·∫´u nhi√™n)", 
                options=authors_list, 
                key="author_B_random_val"
            )

    elif input_method == "H√†ng lo·∫°t t·ª´ file":
        st.subheader("D·ª± ƒëo√°n h√†ng lo·∫°t t·ª´ file")
        uploaded_file = st.file_uploader("T·∫£i l√™n danh s√°ch c·∫∑p t√°c gi·∫£ (.csv ho·∫∑c .xlsx)", type=["csv", "xlsx"])
    name1 = author_A.hasName if hasattr(author_A, 'hasName') else author_A
    name2 = author_B.hasName if hasattr(author_B, 'hasName') else author_B

    if st.button("D·ª± ƒëo√°n"):
        if not onto_data or not review_year:
            st.error("Vui l√≤ng ƒë·∫£m b·∫£o ontology ƒë√£ ƒë∆∞·ª£c n·∫°p v√† c√≥ nƒÉm xem x√©t.")
        else:
            if input_method == "Ng·∫´u nhi√™n" or input_method == "Th·ªß c√¥ng":
                if author_A and author_B:
                    if author_A == author_B:
                        st.error("Vui l√≤ng ch·ªçn hai t√°c gi·∫£ kh√°c nhau.")
                    elif author_A not in author_names or author_B not in author_names:
                        st.error(f"M·ªôt ho·∫∑c c·∫£ hai t√°c gi·∫£ kh√¥ng t·ªìn t·∫°i trong ontology. Vui l√≤ng ki·ªÉm tra l·∫°i.")
                    else:
                        with st.spinner('ƒêang tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng v√† d·ª± ƒëo√°n...'):
                            # X√°c ƒë·ªãnh model_type cho h√†m extract_features_for_pair
                            if model_choice == "Graph Convolutional Netwwork (GCN)":
                                model_type = "GCN"
                                # Khi d√πng GCN c·∫ßn truy·ªÅn th√™m onto_data v√† review_year v√†o load_model
                                model = load_model(model_choice, selected_db, _onto_data=onto_data, _review_year=review_year)
                            else:
                                model_type = "MLP"
                                model = load_model(model_choice, selected_db)
                            if model:
                                st.info(f"ƒêang s·ª≠ d·ª•ng m√¥ h√¨nh: **{model_choice}**")
                                features_df = extract_features_for_pair(onto_data, author_A, author_B, review_year, model_type=model_type)
                                # name1 = author_A.hasName if hasattr(author_A, 'hasName') else author_A
                                # name2 = author_B.hasName if hasattr(author_B, 'hasName') else author_B
                                commonAff = float(features_df.loc[0, 'hasCommonAffiliation']) if 'hasCommonAffiliation' in features_df.columns else 0
                                commonInt = float(features_df.loc[0, 'hasCommonInterest']) if 'hasCommonInterest' in features_df.columns else 0
                                commonPast1 = features_df.loc[0, 'hasPastStatus'] if 'hasPastStatus' in features_df.columns else 0
                                commonPast2 = features_df.loc[0, 'hasPast2'] if 'hasPast2' in features_df.columns else 0
                                commonPast3 = features_df.loc[0, 'hasPast3'] if 'hasPast3' in features_df.columns else 0
                                commonPastTotal = features_df.loc[0, 'hasPastTotal'] if 'hasPastTotal' in features_df.columns else 0
                                explanation = f"Hai t√°c gi·∫£ "
                                if commonAff > 0: 
                                    explanation += f" c√≥ **{commonAff*100:.2f}%** ƒë·ªô t∆∞∆°ng ƒë·ªìng v·ªÅ c∆° quan; "
                                else:
                                    explanation += f"kh√¥ng c√≥ li√™n quan  n∆°i l√†m vi·ªác ho·∫∑c c·ªông t√°c; "
                                if commonInt > 0:
                                    explanation += f"**{commonInt*100:.2f}%** ƒë·ªô t∆∞∆°ng ƒë·ªìng v·ªÅ lƒ©nh v·ª±c quan t√¢m; "
                                else:
                                    explanation += f"kh√¥ng c√≥ lƒ©nh v·ª±c quan t√¢m chung; "
                                if commonPast1 > 0:
                                    explanation += f"ƒë√£ c√πng nhau vi·∫øt **{commonPast1}** b√†i b√°o trong kho·∫£ng th·ªùi gian c√°ch ƒë√¢y 01 nƒÉm; "
                                if commonPast2 > 0:
                                    explanation += f"ƒë√£ c√πng nhau vi·∫øt **{commonPast2}** b√†i b√°o trong kho·∫£ng th·ªùi gian c√°ch ƒë√¢y 02 nƒÉm; "
                                if commonPast3 > 0:
                                    explanation += f"ƒë√£ c√πng nhau vi·∫øt **{commonPast3}** b√†i b√°o trong kho·∫£ng th·ªùi gian c√°ch ƒë√¢y 03 nƒÉm; "
                                explanation += f"t·ªïng c·ªông ƒë√£ c√πng nhau vi·∫øt **{commonPastTotal}** b√†i b√°o trong qu√° kh·ª©."
                                # G·∫Øn t√™n c·∫∑p v√†o DataFrame ƒë·ªÉ GCNWrapper d√πng
                                if model_type == "GCN":
                                    features_df.author_names_pair = (author_A, author_B)
                                if features_df is not None:
                                    st.subheader("ƒê·∫∑c tr∆∞ng ƒë√£ tr√≠ch xu·∫•t")
                                    st.dataframe(features_df, hide_index=True)
                                    prob = predict_link_prob(features_df, model)
                                    if prob >= prob_threshold:
                                        explanation += f" V·ªõi x√°c su·∫•t **{prob*100:.2f}%** cao h∆°n ng∆∞·ª°ng d·ª± ƒëo√°n, hai t√°c gi·∫£ c√≥ nhi·ªÅu kh·∫£ nƒÉng c·ªông t√°c trong t∆∞∆°ng lai."
                                        st.info(f"**Gi·∫£i th√≠ch:** {explanation}")
                                        st.success(f"Hai t√°c gi·∫£ **{author_A}** v√† **{author_B}** C√ì nhi·ªÅu kh·∫£ nƒÉng c·ªông t√°c trong t∆∞∆°ng lai")
                                        # st.success(f"Hai t√°c gi·∫£ **{author_A}** - (**{name1}**) v√† **{author_B}** - (**{name2}**) c√≥ nhi·ªÅu kh·∫£ nƒÉng c·ªông t√°c trong t∆∞∆°ng lai")

                                    else:
                                        explanation += f" V·ªõi x√°c su·∫•t **{prob*100:.2f}%** th·∫•p h∆°n ng∆∞·ª°ng d·ª± ƒëo√°n, hai t√°c gi·∫£ c√≥ th·ªÉ kh√¥ng c·ªông t√°c trong t∆∞∆°ng lai."
                                        st.info(f"**Gi·∫£i th√≠ch:** {explanation}")
                                        st.warning(f"Hai t√°c gi·∫£ **{author_A}** v√† **{author_B}** c√≥ th·ªÉ KH√îNG c·ªông t√°c trong t∆∞∆°ng lai")
                                else:
                                    st.error("Kh√¥ng th·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng. Vui l√≤ng ki·ªÉm tra l·∫°i t√™n t√°c gi·∫£ v√† d·ªØ li·ªáu ontology.")
                            else:
                                st.error("Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh. Vui l√≤ng ki·ªÉm tra file m√¥ h√¨nh.")
                else:
                    st.error("Vui l√≤ng nh·∫≠p ho·∫∑c ch·ªçn √≠t nh·∫•t hai t√°c gi·∫£.")
            
            elif input_method == "H√†ng lo·∫°t t·ª´ file":
                if uploaded_file is not None:
                    try:
                        if uploaded_file.name.endswith('.csv'):
                            df = pd.read_csv(uploaded_file)
                        elif uploaded_file.name.endswith('.xlsx'):
                            df = pd.read_excel(uploaded_file)
                        else:
                            st.error("ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£.")
                            df = None
                        
                        if df is not None:
                            st.dataframe(df)
                            st.info("ƒêang x·ª≠ l√Ω d·ª± ƒëo√°n h√†ng lo·∫°t...")
                            
                            required_cols = ['author_A', 'author_B']
                            if not all(col in df.columns for col in required_cols):
                                st.error("File t·∫£i l√™n ph·∫£i ch·ª©a c√°c c·ªôt 'author_A' v√† 'author_B'.")
                            else:
                                with st.spinner("ƒêang x·ª≠ l√Ω d·ª± ƒëo√°n..."):
                                    model = load_model(model_choice, selected_db)
                                    if model:
                                        predictions = []
                                        for index, row in df.iterrows():
                                            author_A_batch = row['author_A']
                                            author_B_batch = row['author_B']
                                            features = extract_features_for_pair(onto_data, author_A_batch, author_B_batch, review_year)
                                            if features is not None:
                                                prob = predict_link_prob(features, model)
                                                predictions.append(prob)
                                            else:
                                                predictions.append(None)
                                        
                                        df['probability'] = predictions
                                        
                                        st.subheader("K·∫øt qu·∫£ d·ª± ƒëo√°n h√†ng lo·∫°t")
                                        st.dataframe(df)

                                        csv_output = df.to_csv(index=False).encode('utf-8')
                                        st.download_button(
                                            label="T·∫£i xu·ªëng k·∫øt qu·∫£ d·ª± ƒëo√°n (.csv)",
                                            data=csv_output,
                                            file_name='ket_qua_du_doan_hang_loat.csv',
                                            mime='text/csv'
                                        )

                                    else:
                                        st.error("Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh. Vui l√≤ng ki·ªÉm tra file m√¥ h√¨nh.")
                    except Exception as e:
                        st.error(f"ƒê√£ x·∫£y ra l·ªói khi ƒë·ªçc file: {e}")
                else:
                    st.warning("Vui l√≤ng t·∫£i l√™n m·ªôt file ƒë·ªÉ d·ª± ƒëo√°n h√†ng lo·∫°t.")

# Tab 2: Tra c·ª©u Th√¥ng tin
with tab2:
    st.header("2. Tra c·ª©u th√¥ng tin & G·ª£i √Ω c·ªông t√°c üîç")
    if not onto_data:
        st.warning("Ch∆∞a c√≥ Ontology n√†o ƒë∆∞·ª£c n·∫°p. Vui l√≤ng ch·ªçn Ontology ·ªü thanh b√™n ƒë·ªÉ s·ª≠ d·ª•ng ch·ª©c nƒÉng n√†y.")
    else:
        # reset_authors_list()
        # Ch·ªçn t√°c gi·∫£ c·∫ßn tra c·ª©u
        search_author_name = st.selectbox("Ch·ªçn t√°c gi·∫£ c·∫ßn tra c·ª©u:", options=authors_list)
        
        with st.spinner("ƒêang t√¨m ki·∫øm..."):
            # L·∫•y th√¥ng tin t√°c gi·∫£
            selected_author = next((a for a in authors_full_list if a.name == search_author_name), None)

        # Hi·ªÉn th·ªã th√¥ng tin
        if selected_author:
            st.subheader("Th√¥ng tin chi ti·∫øt")
            st.write(f"**NƒÉm xem x√©t:** **{review_year}**")
            st.write(f"**T√™n:** {selected_author.hasName}")
            
            # 1. T√™n (hasName) v√† ƒë∆°n v·ªã (hasAffiliation)
            if selected_author.hasAffiliation:
                st.write(f"**ƒê∆°n v·ªã:** {selected_author.hasAffiliation}")
            else:
                st.write("**ƒê∆°n v·ªã:** Kh√¥ng c√≥ th√¥ng tin.")

            # 2. Lƒ©nh v·ª±c quan t√¢m (hasInterests)
            if selected_author.hasInterestArea:
                st.write(f"**Lƒ©nh v·ª±c quan t√¢m:** {', '.join(selected_author.hasInterestArea)}")
            else:
                st.write("**Lƒ©nh v·ª±c quan t√¢m:** Kh√¥ng c√≥ th√¥ng tin.")

            # 3. S·ªë b√†i b√°o tr∆∞·ªõc nƒÉm xem x√©t
            publications_before_review = []
            if selected_author.authored and review_year:
                publications_before_review = [
                    pub for pub in selected_author.authored if int(pub.hasPublicationYear) < review_year
                ]
                st.write(f"**S·ªë b√†i b√°o tr∆∞·ªõc nƒÉm {review_year}:** {len(publications_before_review)}")
            else:
                st.write("**S·ªë b√†i b√°o:** Kh√¥ng c√≥ th√¥ng tin ho·∫∑c thi·∫øu nƒÉm xem x√©t.")

            # 4. C·ªông t√°c vi√™n tr∆∞·ªõc nƒÉm xem x√©t (hasCoAuthors)
            co_authors_before_review = set()
            if selected_author.authored and review_year:
                for pub in publications_before_review:
                    if pub.authored:
                        for co_author_iri in pub.authored:
                            if co_author_iri != selected_author.iri:
                                co_author_name = co_author_iri.name
                                co_authors_before_review.add(co_author_name)
            
            if co_authors_before_review:
                st.write(f"**ƒê·ªìng t√°c gi·∫£ tr∆∞·ªõc nƒÉm {review_year}:**")
                st.write(f"S·ªë l∆∞·ª£ng ƒë·ªìng t√°c gi·∫£: {len(co_authors_before_review)}")
                coAuthorString = "; ".join(co_authors_before_review)
                st.write(coAuthorString)
            else:
                st.write("**ƒê·ªìng t√°c gi·∫£ tr∆∞·ªõc nƒÉm xem x√©t:** Kh√¥ng c√≥.")
        else:
            st.info("Vui l√≤ng ch·ªçn m·ªôt t√°c gi·∫£ ƒë·ªÉ xem th√¥ng tin chi ti·∫øt.")
        N = min(100,len(author_names))
        author_namesN = random.choices(author_names,k= N)
        print(f"S·ªë t√°c gi·∫£ quan t√¢m: {len(author_namesN)}")
        st.subheader("G·ª£i √Ω c·ªông t√°c")
        if st.button("G·ª£i √Ω"):
            if not onto_data or not review_year:
                st.error("Vui l√≤ng ƒë·∫£m b·∫£o r·∫±ng ontology ƒë√£ ƒë∆∞·ª£c t·∫£i l√™n t∆∞∆°ng ·ª©ng v·ªõi nƒÉm xem x√©t.")
            else:
                with st.spinner("T√¨m ki·∫øm c·ªông t√°c vi√™n ti·ªÅm nƒÉng..."):
                    # Truy·ªÅn th√™m onto_data v√† review_year n·∫øu l√† GCN
                    if model_choice == "Graph Convolutional Netwwork (GCN)":
                        suggestion_model = load_model(model_choice, selected_db, _onto_data=onto_data, _review_year=review_year)
                    else:
                        suggestion_model = load_model(model_choice, selected_db)
                    if suggestion_model:
                        suggestions_df = suggest_collaborators(search_author_name, suggestion_model, author_namesN, onto_data, review_year)
                        if not suggestions_df.empty:
                            st.write("ƒê·ªÅ xu·∫•t c·ªông v·ªõi:")
                            suggestions_df.insert(0, "STT", suggestions_df.index + 1)
                            suggestions_df['X√°c su·∫•t c·ªông t√°c (float)'] = suggestions_df['X√°c su·∫•t c·ªông t√°c'].str.rstrip('%').astype('float') / 100
                            suggestions_df['Kh·∫£ nƒÉng c·ªông t√°c t∆∞∆°ng lai'] = suggestions_df['X√°c su·∫•t c·ªông t√°c (float)'].apply(
                                lambda x: "C√≥" if x >= prob_threshold else "Kh√¥ng"
                            )
                            st.dataframe(suggestions_df[['STT', 'T√™n T√°c gi·∫£', 'Kh·∫£ nƒÉng c·ªông t√°c t∆∞∆°ng lai', 'X√°c su·∫•t c·ªông t√°c']], hide_index=True)
                        else:
                            st.write("Kh√¥ng t√¨m th·∫•y ƒë·ªÅ xu·∫•t.")
                    else:
                        st.error("Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh. Vui l√≤ng ki·ªÉm tra l·∫°i file m√¥ h√¨nh.")
# Tab 3: Qu·∫£n tr·ªã
with tab3:
    st.header("3. Qu·∫£n tr·ªã D·ªØ li·ªáu & M√¥ h√¨nh üõ†Ô∏è")
    st.info("ƒê√¢y l√† khu v·ª±c d√†nh cho qu·∫£n tr·ªã vi√™n")
    
    st.subheader("Qu·∫£n l√Ω Ontology")
    if st.button("N·∫°p l·∫°i Ontology"):
        st.cache_data.clear()
        st.success("ƒê√£ x√≥a cache. Vui l√≤ng l√†m m·ªõi trang ƒë·ªÉ n·∫°p l·∫°i ontology.")
    
    st.subheader("Qu·∫£n l√Ω M√¥ h√¨nh")
    uploaded_model = st.file_uploader("T·∫£i l√™n M√¥ h√¨nh m·ªõi (.pkl)", type="pkl")
    if uploaded_model:
        st.success("ƒê√£ t·∫£i l√™n m√¥ h√¨nh m·ªõi.")

# Tab 4: Xu·∫•t B√°o c√°o
with tab4:
    st.header("4. Xu·∫•t B√°o c√°o üìÑ")
    st.subheader("Xu·∫•t k·∫øt qu·∫£ d·ª± ƒëo√°n")
    st.write("N·ªôi dung b√°o c√°o s·∫Ω ƒë∆∞·ª£c hi·ªÉn th·ªã ·ªü ƒë√¢y.")
    
    csv_data = "author_a,author_b,probability\nJohn,Jane,0.85"
    st.download_button(
        label="T·∫£i xu·ªëng d·ªØ li·ªáu CSV",
        data=csv_data,
        file_name='ket_qua_du_doan.csv',
        mime='text/csv'
    )
    
    st.button("T·∫°o b√°o c√°o PDF/Word (Ch·ª©c nƒÉng n√¢ng cao)")
